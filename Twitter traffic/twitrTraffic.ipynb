{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk, re\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                  created_at  \\\n",
      "0  834019605102358528  ['2017-02-21', '12:39:03']   \n",
      "1  834019562114936834  ['2017-02-21', '12:38:53']   \n",
      "2  834016888372023296  ['2017-02-21', '12:28:16']   \n",
      "3  834004809422356483  ['2017-02-21', '11:40:16']   \n",
      "4  833913312471560192  ['2017-02-21', '05:36:41']   \n",
      "\n",
      "                                                text  \n",
      "0  Siddalinghaiah Jn towards Richmond circle on V...  \n",
      "1  Peak hour traffic at Shanthala Jn towards Cott...  \n",
      "2  Peak hour traffic at Police thimmaiah Crl towa...  \n",
      "3  Peak hour traffic at K.R circle towards Hudson...  \n",
      "4  #TrafficAlert Protest at Town-hall by a politi...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    siddalinghaiah jn towards richmond circle on v...\n",
       "1    peak hour traffic at shanthala jn towards cott...\n",
       "2    peak hour traffic at police thimmaiah crl towa...\n",
       "3    peak hour traffic at k.r circle towards hudson...\n",
       "4    \u0001 protest at town-hall by a political party. p...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets.csv contains the data\n",
    "T_tweets = pd.read_csv('tweets.csv')\n",
    "# how the actual data looks like\n",
    "print(T_tweets.head())\n",
    "\n",
    "# Dropping all the columns except the 'text' column\n",
    "T_tweets.drop(T_tweets.columns[[0,1]], axis=1, inplace=True)\n",
    "\n",
    "# cleaning data\n",
    "T_tweets = T_tweets['text'].str.lower()\n",
    "T_tweets = T_tweets.replace(['((www\\.[^\\s]+)|(https?://[^\\s]+))'],['URL'],regex=True)\n",
    "T_tweets = T_tweets.replace(['[\\s]+'], [' '],regex=True)\n",
    "T_tweets = T_tweets.replace(['#([^\\s]+)'], ['\\1'],regex=True)\n",
    "\n",
    "T_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to work with lists instead of the dataframes\n",
    "tweets = []\n",
    "for i in T_tweets:\n",
    "    tweets.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading nltk stopwords that doesn't have any significance in searching, such as i, me, my, etc.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in tweets:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112183\n",
      "112183\n"
     ]
    }
   ],
   "source": [
    "print(len(totalvocab_stemmed))\n",
    "print(len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 112183 items in vocab_frame\n",
      "                         words\n",
      "siddalinghaiah  siddalinghaiah\n",
      "jn                          jn\n",
      "toward                 towards\n",
      "richmond              richmond\n",
      "circl                   circle\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 24 ms, total: 7.81 s\n",
      "Wall time: 7.81 s\n",
      "(7794, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the result of this block takes a while to show\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(tweets) #fit the vectorizer to synopses\n",
    "\n",
    "# (100, 563) means the matrix has 100 rows and 563 columns\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 0 ns, total: 2.8 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "# doing the actual k-means culstering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()\n",
    "# clusters show which cluster (0-4) each of the 100 synoposes belongs to\n",
    "print(len(clusters))\n",
    "#print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster  rank                                             tweets\n",
      "3        3     1  siddalinghaiah jn towards richmond circle on v...\n",
      "2        2     2  peak hour traffic at shanthala jn towards cott...\n",
      "2        2     3  peak hour traffic at police thimmaiah crl towa...\n",
      "2        2     4  peak hour traffic at k.r circle towards hudson...\n",
      "0        0     5  \u0001 protest at town-hall by a political party. p...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    2833\n",
       "2    2404\n",
       "0    1303\n",
       "3    1254\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = [int(x) for x in range(1, len(tweets)+1)]\n",
    "\n",
    "films = { 'rank': ranks, 'tweets': tweets, 'cluster': clusters}\n",
    "\n",
    "frame = pd.DataFrame(films, index = [clusters])\n",
    "\n",
    "print(frame.head())\n",
    "\n",
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0    3959.650806\n",
       "1    4082.456760\n",
       "2    3757.562396\n",
       "3    3683.341308\n",
       "Name: rank, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = frame['rank'].groupby(frame['cluster']) # groupby cluster for aggregation purposes\n",
    "\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0 words: b'traffic', b'road', b'circle', b'url', b'peak', b'peak',\n",
      "\n",
      "Cluster 0 tweets:\n",
      " \u0001 protest at town-hall by a political party. please avoid all roads leading to townhall. congestion expected from 11am to 1pm., slow moving traffic at tin factory towards kr puram, k.h double road shanti nagar towards richmond circle., good morning to all btp friends. follow traffic rules for your safety.,  , \"\u0001, today cubbon park will be closed, no vehicles will be allowed inside cubbon park, till monday 8am.,\n",
      "\n",
      "Cluster 1 words: b'url', b'traffic', b'road', b'circle', b'hour', b'peak',\n",
      "\n",
      "Cluster 1 tweets:\n",
      " please avoid town hall surrounding roads due to jds protest URL,  . . URL, take a \u0001 before starting auto journey click driver details (display card). it will be helpful if u forget anyth URL, \u0001 URL, today's the \"no honking monday\". lets avoid honking unnecessarily and turn this campaign into a reality. \" URL,\n",
      "\n",
      "Cluster 2 words: b'jn', b'circle', b'peak', b'peak', b'hour', b'peak',\n",
      "\n",
      "Cluster 2 tweets:\n",
      " peak hour traffic at shanthala jn towards cotton pet main rd, maharani college jn towards kr circle on seshadri rd &amp; .., peak hour traffic at police thimmaiah crl towards chalukya circle on rajbhavan rd, sbm circle towards majestic on kg road &amp; hal twrds domlur, peak hour traffic at k.r circle towards hudson circle on nrupathunga road, town hall jn from s.j.p road &amp; dalmia jn from jd mara jn on bg rd, peak hour traffic at h.a.l towards domlur, basaweshwara circle towards cauvery theater circle, madiwala from dairy circle., peak hour traffic at hoody circle towards mahadevapura, freedom park jn towards k.r circle, town hall jn from s.j.p road.,\n",
      "\n",
      "Cluster 3 words: b'jn', b'road', b'circle', b'traffic', b'url', b'peak',\n",
      "\n",
      "Cluster 3 tweets:\n",
      " siddalinghaiah jn towards richmond circle on vittal mallya road, slow moving traffic at agara jn towards bellandur, mysore road at rajarajeshwarinagara entrance and at r.v college jn., and also jayadeva hospital jn towards jeedimara jn on bannerghatta road., slow moving traffic at phoenix mall jn towards hoody circle due to bus breakdown., and also slow moving traffic at yelahanka air force jn towards city due to air show.,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d tweets:\" % i, end='\\n')\n",
    "    for title in frame.ix[i]['tweets'].head().tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
