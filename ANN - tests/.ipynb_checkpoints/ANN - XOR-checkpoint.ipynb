{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1318583 ]\n",
      " [ 1.43516079]]\n",
      "[ 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Training example\n",
    "training_set_inputs = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "training_set_outputs = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# For AND function:- training_set_outputs = np.array([[0], [1], [0], [0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid (x): return 1/(1 + np.exp(-x))      # activation function \n",
    "def sigmoid_(x): return x * (1 - x)             # activation derivative function\n",
    "    \n",
    "input_synaptic_weights = np.random.random(size=(2, 1))\n",
    "#output_synaptic_weights = np.random.random(size=(3, 1))\n",
    "\n",
    "\n",
    "\n",
    "for iteration in range(10000):\n",
    "    l1 = sigmoid(np.dot(training_set_inputs, input_synaptic_weights))\n",
    "    #l2 = sigmoid(np.dot(l1, output_synaptic_weights))\n",
    "    \n",
    "    error = training_set_outputs - l1\n",
    "    delta_output = error * sigmoid_(l1)\n",
    "    #delta_hidden = np.dot(delta_output, output_synaptic_weights.T) * sigmoid_(l1)\n",
    "    \n",
    "    input_synaptic_weights += l1.T.dot(delta_output)\n",
    "    #output_synaptic_weights += l2.T.dot(delta_hidden)\n",
    "    \n",
    "print(input_synaptic_weights)\n",
    "\n",
    "#Test input\n",
    "input = np.array([0,0])\n",
    "\n",
    "#prediction\n",
    "print(sigmoid(np.dot(input,input_synaptic_weights)))\n",
    "\n",
    "\n",
    "##THE SAME MODEL GIVES VERY BAD RESULTS FOR INPUTS TO TEST XOR FUNCTION\n",
    "##A HIDDEN LAYER IS REQUIRED FOR THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08710285]\n",
      " [ 0.08154373]\n",
      " [ 0.9800709 ]\n",
      " [ 0.98007244]]\n",
      "[ 0.72712264]\n"
     ]
    }
   ],
   "source": [
    "##-----------------------XOR---------------------------\n",
    "\n",
    "\n",
    "#Training example\n",
    "training_set_inputs = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "training_set_outputs = np.array([[0], [0], [1], [1]])\n",
    "\n",
    "# For AND function:- training_set_outputs = np.array([[0], [1], [0], [0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid (x): return 1/(1 + np.exp(-x))      # activation function \n",
    "def sigmoid_(x): return x * (1 - x)             # activation derivative function\n",
    "def cost(y, t): return ((t - y)**2).sum()\n",
    "\n",
    "input_synaptic_weights = np.random.random(size=(2, 2))\n",
    "output_synaptic_weights = np.random.random(size=(2, 1))\n",
    "\n",
    "for iteration in range(10000):\n",
    "    l1 = sigmoid(np.dot(training_set_inputs, input_synaptic_weights))\n",
    "    l2 = (np.dot(l1, output_synaptic_weights))\n",
    "    \n",
    "    error = training_set_outputs - l2\n",
    "    delta_output = error * 0.1\n",
    "    output_synaptic_weights += l1.T.dot(delta_output)\n",
    "    \n",
    "    delta_hidden = np.dot(delta_output, output_synaptic_weights.T) * sigmoid_(l1)\n",
    "    input_synaptic_weights += training_set_inputs.T.dot(delta_hidden)\n",
    "    \n",
    "    \n",
    "print(l2)\n",
    "\n",
    "#Test input\n",
    "input = np.array([1,0])\n",
    "\n",
    "#prediction     \n",
    "a = sigmoid(np.dot(input,input_synaptic_weights))\n",
    "print(sigmoid(np.dot(a,output_synaptic_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-43-646e06b868c3>:28: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Epoch  0\n",
      "Hypothesis  [[ 0.61634582]\n",
      " [ 0.61762786]\n",
      " [ 0.63777196]\n",
      " [ 0.63928664]]\n",
      "Theta1  [[ 0.06907363  0.74968964]\n",
      " [ 0.13089728 -0.090042  ]]\n",
      "Bias1  [-0.00015473 -0.00014413]\n",
      "Theta2  [[ 0.48597938]\n",
      " [ 0.46479025]]\n",
      "Bias2  [-0.00128254]\n",
      "cost  0.727332\n"
     ]
    }
   ],
   "source": [
    "#--------------------SAME PROBLEM USING TENSOFLOW-------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "x_ = tf.placeholder(tf.float32, shape=[4,2], name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[4,1], name=\"y-input\")\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[1,2])\n",
    "y = tf.placeholder(tf.float32, shape=[1,1])\n",
    "\n",
    "\n",
    "Theta1 = tf.Variable(tf.random_uniform([2,2], -1, 1), name=\"Theta1\")\n",
    "Theta2 = tf.Variable(tf.random_uniform([2,1], -1, 1), name=\"Theta2\")\n",
    "\n",
    "Bias1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "Bias2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "hidden = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)\n",
    "Hypothesis = tf.sigmoid(tf.matmul(hidden, Theta2) + Bias2)\n",
    "\n",
    "cost = tf.reduce_mean(( (y_ * tf.log(Hypothesis)) + \n",
    "        ((1 - y_) * tf.log(1.0 - Hypothesis)) ) * -1)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "XOR_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "XOR_Y = [[0],[1],[1],[0]]\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./logs/xor_log\", sess.graph_def)\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10000):\n",
    "        summary = sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('Epoch ', i)\n",
    "            print('Hypothesis ', sess.run(Hypothesis, feed_dict={x_: XOR_X, y_: XOR_Y}))\n",
    "            print('Theta1 ', sess.run(Theta1))\n",
    "            print('Bias1 ', sess.run(Bias1))\n",
    "            print('Theta2 ', sess.run(Theta2))\n",
    "            print('Bias2 ', sess.run(Bias2))\n",
    "            print('cost ', sess.run(cost, feed_dict={x_: XOR_X, y_: XOR_Y}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
